#summary Helgrind reports explained on examples 

The goal of this page is to explain users how to read and understand the Helgrind reports.

Simple races could be obvious once you get a report from Helgrind.
However in some cases getting a Helgrind report is just a beginning of the bug analysis.
Here we show several exemplary races with corresponding reports and explain how to analyze the races in each case.

*TODO*: Under construction! 

All the examples on this page are taken from RacecheckUnittest. 

=Simple example=
==Source code==
The simplest example of a data race is *test301* of RacecheckUnittest. 
{{{
3630 int     GLOB = 0;
3631
3632 Mutex MU1;
3633 Mutex MU2;
3634 void Worker1() { MU1.Lock(); GLOB=1; MU1.Unlock(); }
3635 void Worker2() { MU2.Lock(); GLOB=1; MU2.Unlock(); }
3636
3637 void Run() {
3638   printf("test301: simple race.\n");
3639   MyThread t1(Worker1), t2(Worker2);
3640   t1.Start();
3641   t2.Start();
3642   t1.Join();   t2.Join();
3643 }
}}}

Note that the global variable `GLOB` is accessed in two threads, while not holding any *common* lock. 

==Running Helgrind==
{{{
% valgrind --tool=helgrind ./a.out 301
==21787== Thread #2 was created
==21787==    at 0x3D840B5E60: clone (in /lib64/tls/libc-2.3.5.so)
==21787==    by 0x4C2B0D3: do_clone (in /lib64/tls/libpthread-2.3.5.so)
==21787==    by 0x4C2BAE1: pthread_create@@GLIBC_2.2.5 (in /lib64/tls/libpthread-2.3.5.so)
==21787==    by 0x4A0A601: pthread_create@* hg_intercepts.c:213
==21787==    by 0x40DDB3: MyThread::Start(void*) thread_wrappers_pthread.h:242
==21787==    by 0x403AFF: test301::Run() racecheck_unittest.cc:3640
==21787==    by 0x40A2CC: main racecheck_unittest.cc:144
==21787==
==21787== Thread #4 was created
==21787==    at 0x3D840B5E60: clone (in /lib64/tls/libc-2.3.5.so)
==21787==    by 0x4C2B0D3: do_clone (in /lib64/tls/libpthread-2.3.5.so)
==21787==    by 0x4C2BAE1: pthread_create@@GLIBC_2.2.5 (in /lib64/tls/libpthread-2.3.5.so)
==21787==    by 0x4A0A601: pthread_create@* hg_intercepts.c:213
==21787==    by 0x40DDB3: MyThread::Start(void*) thread_wrappers_pthread.h:242
==21787==    by 0x403B0D: test301::Run() racecheck_unittest.cc:3641
==21787==    by 0x40A2CC: main racecheck_unittest.cc:144
==21787==
==21787== T4: Possible data race during write of size 4 at 0x6197D4
==21787==    at 0x404564: test301::Worker2() racecheck_unittest.cc:3635
==21787==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==21787==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==21787==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==21787==   old state = {Write; #SS=1; #LS=1; S2/T2}
==21787==   new state = {Write; #SS=2; #LS=0; S2/T2, S7/T4}
==21787==   Last consistently used lock for 0x6197D4 was first observed
==21787==    at 0x4A0A4EC: pthread_mutex_init hg_intercepts.c:346
==21787==    by 0x40D0EF: Mutex::Mutex() thread_wrappers_pthread.h:149
==21787==    by 0x402928: __static_initialization_and_destruction_0(int, int) racecheck_unittest.cc:3632
==21787==    by 0x4029BA: _GLOBAL__I_MU racecheck_unittest.cc:3692
==21787==    by 0x40FA69: (within a.out)
==21787==    by 0x401552: (within a.out)
==21787==    by 0x40F9C7: (within a.out)
==21787==    by 0x40FA10: __libc_csu_init (in a.out)
==21787==    by 0x3D8401CA75: (below main) (in /lib64/tls/libc-2.3.5.so)
==21787==   Address 0x6197D4 is 0 bytes inside data symbol "_ZN7test3014GLOBE"
==21787==
...
==21787== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 2 from 1)
}}}

==Understanding the report==
So, what information do we get from Helgrind? 

=== Threads ===
First of all, we get the information about the threads involved in the data race. 
For each thread a stack trace (context) of it's creation is printed. For each thread this is printed just once. 
{{{
==21787== Thread #2 was created
==21787==    at 0x3D840B5E60: clone (in /lib64/tls/libc-2.3.5.so)
...
==21787==    by 0x403AFF: test301::Run() racecheck_unittest.cc:3640
==21787==    by 0x40A2CC: main racecheck_unittest.cc:144
==21787==
==21787== Thread #4 was created
==21787==    at 0x3D840B5E60: clone (in /lib64/tls/libc-2.3.5.so)
...
==21787==    by 0x403B0D: test301::Run() racecheck_unittest.cc:3641
==21787==    by 0x40A2CC: main racecheck_unittest.cc:144

}}}

=== Data race == 
The report about the data race itself starts with 
{{{
==21787== T4: Possible data race during write of size 4 at 0x6197D4
}}}
Here we have the thread id `T4`, access type (`read` or `write`), size in bytes, and the address of the memory location. 

Next, the stack trace (context) of the access follows: 
{{{
==21787==    at 0x404564: test301::Worker2() racecheck_unittest.cc:3635
==21787==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==21787==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==21787==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
}}}

After the access context we get the old and the new states of this memory location (for details about states refer to [MSMProp1]).
{{{
==21787==   old state = {Write; #SS=1; #LS=1; S2/T2}
==21787==   new state = {Write; #SS=2; #LS=0; S2/T2, S7/T4}
}}}
In this particular report we see that before the last access the state contains only one thread segment and now it has two thread segments. 
Since the new state is `Write` and the lockset is empty, we have a race. 

Next line shows whether this memory location has been procted by any lock. 
In this case, this is the lock `MU1`, the report gives the context of `MU1` creation.
{{{
==21787==   Last consistently used lock for 0x6197D4 was first observed
==21787==    at 0x4A0A4EC: pthread_mutex_init hg_intercepts.c:346
==21787==    by 0x40D0EF: Mutex::Mutex() thread_wrappers_pthread.h:149
==21787==    by 0x402928: __static_initialization_and_destruction_0(int, int) racecheck_unittest.cc:3632
...
}}}

The last line shows us the debug information for the memory location. If we are lucky to get a race on a global variable, we will see it's name. 
{{{
==21787==   Address 0x6197D4 is 0 bytes inside data symbol "_ZN7test3014GLOBE"
}}}

=Using traces (when the data race happens twice)=
So, in the the first example the data race is quite obvious from the report (isn't it?). 
We have a global variable which is not protected by any common lock. 

However, in most real cases the racey memory is hidden somewhere deep inside 
the data structures and accessed from many different places. 
Analyzing the data race by just looking at the race context is impossible. 
However, if the race on the given memory location happens at least twice, 
we can use *access traces* to understand the race. 

== Source code ==
In the following test we have many memory accesses and much less obvious race (one access is mistakenly protected by a wrong lock). 
{{{
3649 // In this test we have many different accesses to GLOB and only one access
3650 // is not synchronized properly.
3651 int     GLOB = 0;
3652
3653 Mutex MU1;
3654 Mutex MU2;
3655 void Worker() {
3656   for(int i = 0; i < 100; i++) {
3657     switch(i % 4) {
3658       case 0:
3659         // This read is protected correctly.
3660         MU1.Lock(); CHECK(GLOB >= 0); MU1.Unlock();
3661         break;
3662       case 1:
3663         // Here we used the wrong lock! The reason of the race is here.
3664         MU2.Lock(); CHECK(GLOB >= 0); MU2.Unlock();
3665         break;
3666       case 2:
3667         // This read is protected correctly.
3668         MU1.Lock(); CHECK(GLOB >= 0); MU1.Unlock();
3669         break;
3670       case 3:
3671         // This write is protected correctly.
3672         MU1.Lock(); GLOB++; MU1.Unlock();
3673         break;
3674     }
3675     // sleep a bit so that the threads interleave
3676     // and the race happens at least twice.
3677     usleep(100);
3678   }
3679 }
3680
3681 void Run() {
3682   printf("test302: Complex race that happens twice.\n");
3683   MyThread t1(Worker), t2(Worker);
3684   t1.Start();
3685   t2.Start();
3686   t1.Join();   t2.Join();
3687 }
}}}

== Running Helgrind == 
{{{
% valgrind --tool=helgrind ./a.out 302
==29853== Thread #2 was created
...
==29853== Thread #4 was created
...
==29853== T2: Possible data race during write of size 4 at 0x619C3C
==29853==    at 0x404E0D: test302::Worker() racecheck_unittest.cc:3672
==29853==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==29853==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==29853==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==29853==   old state = {Read; #SS=2; #LS=0; S11/T4, S13/T2}
==29853==   new state = {Write; #SS=2; #LS=0; S11/T4, S13/T2}
==29853==   Last consistently used lock for 0x619C3C was first observed
==29853==    at 0x4A0A4EC: pthread_mutex_init hg_intercepts.c:346
==29853==    by 0x40D37F: Mutex::Mutex() thread_wrappers_pthread.h:149
==29853==    by 0x4029CB: __static_initialization_and_destruction_0(int, int) racecheck_unittest.cc:3654
...
==29853==   Address 0x619C3C is 0 bytes inside data symbol "_ZN7test3024GLOBE"
==29853== 
==29853== TRACE {{{: Access = {0x619C3C S13/T2 write} State = {Write; #SS=1; #LS=1; S13/T2}
==29853==  Access stack trace:
==29853==    at 0x404E0D: test302::Worker() racecheck_unittest.cc:3672
==29853==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==29853==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==29853==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==29853==  Locks held:
==29853==    L:0x402117E00/0x619C70
==29853==    at 0x4A078D5: pthread_mutex_lock hg_intercepts.c:408
==29853==    by 0x40E19E: Mutex::Lock() thread_wrappers_pthread.h:156
==29853==    by 0x404E03: test302::Worker() racecheck_unittest.cc:3672
...
==29853== }}}
==29853== 
==29853== TRACE {{{: Access = {0x619C3C S17/T2 read} State = {Read; #SS=1; #LS=1; S17/T2}
==29853==  Access stack trace:
==29853==    at 0x404D56: test302::Worker() racecheck_unittest.cc:3660
==29853==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==29853==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==29853==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==29853==  Locks held:
==29853==    L:0x402117E00/0x619C70
==29853==    at 0x4A078D5: pthread_mutex_lock hg_intercepts.c:408
==29853==    by 0x40E19E: Mutex::Lock() thread_wrappers_pthread.h:156
==29853==    by 0x404D55: test302::Worker() racecheck_unittest.cc:3660
...
==29853== }}}
==29853== 
==29853== TRACE {{{: Access = {0x619C3C S15/T4 read} State = {Read; #SS=2; #LS=1; S15/T4, S17/T2}
==29853==  Access stack trace:
==29853==    at 0x404DCB: test302::Worker() racecheck_unittest.cc:3668
==29853==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==29853==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==29853==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==29853==  Locks held:
==29853==    L:0x402117E00/0x619C70
==29853==    at 0x4A078D5: pthread_mutex_lock hg_intercepts.c:408
==29853==    by 0x40E19E: Mutex::Lock() thread_wrappers_pthread.h:156
==29853==    by 0x404DCA: test302::Worker() racecheck_unittest.cc:3668
...
==29853== }}}
==29853== 
==29853== TRACE {{{: Access = {0x619C3C S19/T2 read} State = {Read; #SS=2; #LS=0; S15/T4, S19/T2}
==29853==  Access stack trace:
==29853==    at 0x404D92: test302::Worker() racecheck_unittest.cc:3664
==29853==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==29853==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==29853==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==29853==  Locks held:
==29853==    L:0x402117F18/0x619CD0
==29853==    at 0x4A078D5: pthread_mutex_lock hg_intercepts.c:408
==29853==    by 0x40E19E: Mutex::Lock() thread_wrappers_pthread.h:156
==29853==    by 0x404D91: test302::Worker() racecheck_unittest.cc:3664
...
==29853== }}}
==29853== 
==29853== TRACE {{{: Access = {0x619C3C S21/T4 read} State = {Read; #SS=2; #LS=0; S19/T2, S21/T4}
==29853==  Access stack trace:
==29853==    at 0x404E04: test302::Worker() racecheck_unittest.cc:3672
==29853==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==29853==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==29853==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==29853==  Locks held:
==29853==    L:0x402117E00/0x619C70
==29853==    at 0x4A078D5: pthread_mutex_lock hg_intercepts.c:408
==29853==    by 0x40E19E: Mutex::Lock() thread_wrappers_pthread.h:156
==29853==    by 0x404E03: test302::Worker() racecheck_unittest.cc:3672
...
==29853== }}}
==29853== 
==29853== TRACE {{{: Access = {0x619C3C S21/T4 write} State = {Write; #SS=2; #LS=0; S19/T2, S21/T4}
==29853==  Access stack trace:
==29853==    at 0x404E0D: test302::Worker() racecheck_unittest.cc:3672
==29853==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==29853==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==29853==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==29853==  Locks held:
==29853==    L:0x402117E00/0x619C70
==29853==    at 0x4A078D5: pthread_mutex_lock hg_intercepts.c:408
==29853==    by 0x40E19E: Mutex::Lock() thread_wrappers_pthread.h:156
==29853==    by 0x404E03: test302::Worker() racecheck_unittest.cc:3672
...
==29853== }}}
==29853== 
==29853== Race on 0x619C3C is found again
==29853== 
==29853== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 8 from 3)
}}}

== Understanding the traces ==
So, the race report itself gives us very little information: 
{{{
==29853== T2: Possible data race during write of size 4 at 0x619C3C
==29853==    at 0x404E0D: test302::Worker() racecheck_unittest.cc:3672
==29853==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==29853==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==29853==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==29853==   old state = {Read; #SS=2; #LS=0; S11/T4, S13/T2}
==29853==   new state = {Write; #SS=2; #LS=0; S11/T4, S13/T2}
==29853==   Last consistently used lock for 0x619C3C was first observed
==29853==    at 0x4A0A4EC: pthread_mutex_init hg_intercepts.c:346
==29853==    by 0x40D37F: Mutex::Mutex() thread_wrappers_pthread.h:149
==29853==    by 0x4029CB: __static_initialization_and_destruction_0(int, int) racecheck_unittest.cc:3654
...
}}}

We see that the racey memory is `GLOB`, but we don't see why there is a race: this access to `GLOB` is protected by a lock. 
But once the race is reported, Helgrind enables *access tracing* for this particular memory location.
The state of this memory location is reset to `{Accesstype,currS, currLS}`, i.e. all the previous accesses are forgotten. 

Each trace contains:
  * Access information: `Access = {0x6197D4 S7/T4 write}` (address, segment/thread, access type)
  * State of the memory location (right after the access): `State = {Write; #SS=1; #LS=1; S7/T4}` (See MSMProp1).
  * Access stack trace (context).
  * List of locks held during the access (if the access is `read`, RW- and R-locks are showen; for `write` only RW-locks are showen). For each lock the report shows the context of the lock acquisition.

In this test case we are lucky -- the race happened twice: 
{{{
==29853== Race on 0x619C3C is found again
}}}
After the race has been found twice, the memory tracing is disabled forever for this memory location. 


Let's look at the lines starting with `TRACE`: 
{{{
1: TRACE {{{: Access = {0x619C3C S13/T2 write} State = {Write; #SS=1; #LS=1; S13/T2}
2: TRACE {{{: Access = {0x619C3C S17/T2 read} State = {Read; #SS=1; #LS=1; S17/T2}
3: TRACE {{{: Access = {0x619C3C S15/T4 read} State = {Read; #SS=2; #LS=1; S15/T4, S17/T2}
4: TRACE {{{: Access = {0x619C3C S19/T2 read} State = {Read; #SS=2; #LS=0; S15/T4, S19/T2}
5: TRACE {{{: Access = {0x619C3C S21/T4 read} State = {Read; #SS=2; #LS=0; S19/T2, S21/T4}
6: TRACE {{{: Access = {0x619C3C S21/T4 write} State = {Write; #SS=2; #LS=0; S19/T2, S21/T4}
}}}

We see that the Lockset became empty after the access 4 (access 3 has `#LS=1`, access 4 has `#LS=0`).
Let's look at the accesses 3 and 4: 
{{{
==29853== TRACE {{{: Access = {0x619C3C S15/T4 read} State = {Read; #SS=2; #LS=1; S15/T4, S17/T2}
==29853==  Access stack trace:
==29853==    at 0x404DCB: test302::Worker() racecheck_unittest.cc:3668
==29853==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==29853==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==29853==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==29853==  Locks held:
==29853==    L:0x402117E00/0x619C70
==29853==    at 0x4A078D5: pthread_mutex_lock hg_intercepts.c:408
==29853==    by 0x40E19E: Mutex::Lock() thread_wrappers_pthread.h:156
==29853==    by 0x404DCA: test302::Worker() racecheck_unittest.cc:3668
...
==29853== }}}

==29853== TRACE {{{: Access = {0x619C3C S19/T2 read} State = {Read; #SS=2; #LS=0; S15/T4, S19/T2}
==29853==  Access stack trace:
==29853==    at 0x404D92: test302::Worker() racecheck_unittest.cc:3664
==29853==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==29853==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==29853==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==29853==  Locks held:
==29853==    L:0x402117F18/0x619CD0
==29853==    at 0x4A078D5: pthread_mutex_lock hg_intercepts.c:408
==29853==    by 0x40E19E: Mutex::Lock() thread_wrappers_pthread.h:156
==29853==    by 0x404D91: test302::Worker() racecheck_unittest.cc:3664
...
==29853== }}}
}}}

We see that during these two accesses we hold two different locks (`MU1` and `MU2`).
We are done! 


Note, that even though the lock was lost during the access 4, 
we can not report a race untill access 6 since reading shared data w/o a lock is not a crime. 


=Using annotations (when the data race happens once)=
Ok, now we know how to analyze races that happen twice (according to TODO 80% of races do happen twice).
But what shall we do if a race never repeats? 

In some cases, source code annotations and a second run of Helgrind helps! 

== Source code == 
Suppose we have a real large application and some memory 
location is accessed once w/o a proper synchronization 
(and many times with proper synchronization).
How to find this unsynchronized access?

{{{
3694 int     GLOB = 0;
3695
3696 Mutex MU;
3697 void Worker1() { CHECK(GLOB >= 0); }
3698 void Worker2() { MU.Lock(); GLOB=1;  MU.Unlock();}
3699
3700 void Run() {
3701   printf("test303: a race that needs annotations.\n");
3702   ANNOTATE_TRACE_MEMORY(&GLOB);
3703   MyThreadArray t(Worker1, Worker2);
3704   t.Start();
3705   t.Join();
3706 }
}}}


== Running Helgrind == 
{{{
% valgrind --tool=helgrind ./a.out 303
==11115== Thread #2 was created
...
==11115== Thread #3 was created
...
==11115== T3: Possible data race during write of size 4 at 0x619BDC
==11115==    at 0x40461E: test303::Worker2() racecheck_unittest.cc:3698
==11115==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==11115==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==11115==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==11115==   old state = {Read; #SS=1; #LS=0; S2/T2}
==11115==   new state = {Write; #SS=2; #LS=0; S2/T2, S4/T3}
==11115==   Location 0x619BDC has never been protected by any lock
==11115==   Address 0x619BDC is 0 bytes inside data symbol "_ZN7test3034GLOBE"
}}}

==Annotating the source code and rerunning Helgrind== 
The report gave us the racey memory location 
(in this case it is a global variable, 
but it could be a field in a heap allocated structure or anything else). 
In many cases the simplets way to analyze the race is to annotate the racey address 
and rerun Helgrind with `--trace-level=2`.

Note that our example is already annotated: 
{{{
3702   ANNOTATE_TRACE_MEMORY(&GLOB);
}}}
The annotation is an executable code and should be inserted before the first use of this memory location (when possible).

So, let's rerun Helgrind:
{{{
% valgrind --tool=helgrind --trace-level=2 ./a.out 303
==13317== ENABLED TRACE {{{: 0x619BDC; S1/T1
==13317==    at 0x4A0AD1D: AnnotateTraceMemory hg_annotations.c:267
==13317==    by 0x404BA5: test303::Run() racecheck_unittest.cc:3702
==13317==    by 0x40A4CE: main racecheck_unittest.cc:144
==13317== }}}
...
==13317== TRACE {{{: Access = {0x619BDC S2/T2 read} State = {Read; #SS=1; #LS=0; S2/T2}
==13317==  Access stack trace:
==13317==    at 0x401BEE: test303::Worker1() racecheck_unittest.cc:3697
==13317==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==13317==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==13317==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==13317==  Locks held:
==13317==    None
==13317== }}}
...
==13317== TRACE {{{: Access = {0x619BDC S4/T3 write} State = {Write; #SS=2; #LS=0; S2/T2, S4/T3}
==13317==  Access stack trace:
==13317==    at 0x40461E: test303::Worker2() racecheck_unittest.cc:3698
==13317==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==13317==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==13317==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==13317==  Locks held:
==13317==    L:0x402118030/0x619C10
==13317==    at 0x4A078D5: pthread_mutex_lock hg_intercepts.c:408
==13317==    by 0x40E110: Mutex::Lock() thread_wrappers_pthread.h:156
==13317==    by 0x40461D: test303::Worker2() racecheck_unittest.cc:3698
==13317==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==13317==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==13317==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==13317== }}}
...
==13317== T3: Possible data race during write of size 4 at 0x619BDC
==13317==    at 0x40461E: test303::Worker2() racecheck_unittest.cc:3698
...
}}}

So, we see the access traces on each access after 
the statement `ANNOTATE_TRACE_MEMORY(&GLOB);` has been executed. 

Looking at these two traces we see that the first access was made 
while not holding any lock. We are done! 


=Using segments (when the data race happens once and annotations do not help)=
TODO

=Possible improvements=
  * Implement Helgrind client requests to attach symbolic names to threads and locks. 
  * TODO

