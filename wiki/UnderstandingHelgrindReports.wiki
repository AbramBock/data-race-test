#summary Helgrind reports explained on examples

The goal of this page is to explain users how to read and understand the Helgrind reports.

Simple races could be obvious once you get a report from Helgrind.
However in some cases getting a Helgrind report is just a beginning of the bug analysis.
Here we show several exemplary races with corresponding reports and explain how to analyze the races in each case.

*TODO*: Under construction! 

All the examples on this page are taken from RacecheckUnittest. 

This page describes the bahavior of the Helgrind variant available from http://data-race-test.googlecode.com/svn/trunk/helgrind. 
The 'official' Helgrind behaves a bit differently.

=Helgrind basics=

TODO *This section is under construction!*

Unfortunately or not, but Helgrind users have to understand how the tool works. 

== Events == 
Helgrind is a dynamic data race detector, meaning that it executes the application being tested. 
While executing an application Helgrind observes certain events, each event happening in some thread. 

The events are: 
  * Memory access
    * Read  {T,A,size}
    * Write {T,A,size}
  * Locking event 
    * !WriteLock {T,L}
    * !ReaderLock {T,L}
    * Unlock {T,L}
  * Happens-before event 
    * Signal {T, H}
    * Wait {T,H}

Legend: 
  * T -- a thread.
  * A -- a memory address.
  * size -- a size of memory access (usually one of 1, 2, 4, 8).
  * L -- a lock object (actually just an address). For example, this could be a `pthread_mutex_t` object.
  * H -- a happens-before synchronization object (again, just an address). For example, this could be a `pthread_cond_t` object.

An example: 

 http://data-race-test.googlecode.com/svn/trunk/msm/exa1.png

In this example we have two threads, *T1* and *T2*, one happens-before synchronization object *H*, one locking object *L* and two memory locations, *A* and *B*. 
*B* is proteceted with lock *L*. 
Ownership of *A* is passed between threads using *H*. 

== Happens-before ==

All *memory access* events of one thread that occur between *happens-before* events form a *segment*. 
We say that that segment S1 happens-before segment S2 (*HB(S1,S2)*) if there is an ordering constraint on these two segments such that the first event of S2 will occur after the last event of S1. For any two segments that belong to the same thread, a segment with the smaller number happens-before a segment with the greater number.
A pair of  *Signal* and *Wait* events with a matching synchronization object *H* creates a so called *happens-before* arc. All segments that occured before *Signal* in the signalling thread happen-before all segments that occur after *Wait* in the wating thread. 
 http://data-race-test.googlecode.com/svn/trunk/msm/exa2.png
In this example: 
  * HB(Seg1, Seg3), HB(Seg2, Seg4) -- segments belong to the same thread
  * HB(Seg1, Seg4) -- happens-before arc due to Signal/Wait pair with a macthing synchronization object.
  * not HB(Seg1, Seg2), not HB(Seg3, Seg4)  -- no ordering constraint.

Happens-before relation is transitive: in the example below, we have HB(Seg1, Seg7).
 http://data-race-test.googlecode.com/svn/trunk/msm/exa3.png

One Signal may be connected to multiple Wait events and vice versa. 
 http://data-race-test.googlecode.com/svn/trunk/msm/exa4.png

A *Barrier* is simply a situation where every thread signals and then every thread waits. 
 http://data-race-test.googlecode.com/svn/trunk/msm/exa5.png


== Lock set == 
Each thread maintains a *lock set* -- a set of locks currently held. 
More precisely, there are two locksets: *RW-lockset* (contains locks that are held in write mode) and *R-lockset* (reader locks).
*R-lockset* is a supreset of *RW-lockset*.

== State machine == 
For each memory location accessed during the execution, Helgrind maintains a *shadow-word*, 
a 64-bit value that encrypts the information about previous accessed to this memory. 
Each memory access changes the shadow-word according to certain rules (the set of rules is called  a *state machine*). 
The contents of the shadow words and the state machine is a subject to change.
The precise desciprtion of the current state machine is at [MSMHelgrind340].





=Simple example=
In this section we show simple examples of racey code and the reports produced by Helgrind.
==Source code==
The simplest example of a data race is *test301* of RacecheckUnittest. 
{{{
4606 int     GLOB = 0;
4607
4608 Mutex MU1;
4609 Mutex MU2;
4610 void Worker1() { MU1.Lock(); GLOB=1; MU1.Unlock(); }
4611 void Worker2() { MU2.Lock(); GLOB=1; MU2.Unlock(); }
4612
4613 void Run() {
4614   printf("test301: simple race.\n");
4615   MyThread t1(Worker1), t2(Worker2);
4616   t1.Start();
4617   t2.Start();
4618   t1.Join();   t2.Join();
4619 }
}}}

Note that the global variable `GLOB` is accessed in two threads, while not holding any *common* lock. 

==Running Helgrind==
{{{
% valgrind --tool=helgrind ./a.out 301
==12739== Thread T3 was created
==12739==    at 0x9F4EE60: clone (in /lib64/tls/libc-2.3.5.so)
==12739==    by 0x94FD0D3: do_clone (in /lib64/tls/libpthread-2.3.5.so)
==12739==    by 0x94FDAE1: pthread_create@@GLIBC_2.2.5 (in /lib64/tls/libpthread-2.3.5.so)
==12739==    by 0x92DBED1: pthread_create@* hg_intercepts.c:213
==12739==    by 0x41622D: MyThread::Start(void*) thread_wrappers_pthread.h:242
==12739==    by 0x4081D2: test301::Run() racecheck_unittest.cc:4616
==12739==    by 0x4170BA: Test::Run() racecheck_unittest.cc:142
==12739==    by 0x40F099: main racecheck_unittest.cc:192
==12739==
==12739== Thread T4 was created
==12739==    at 0x9F4EE60: clone (in /lib64/tls/libc-2.3.5.so)
==12739==    by 0x94FD0D3: do_clone (in /lib64/tls/libpthread-2.3.5.so)
==12739==    by 0x94FDAE1: pthread_create@@GLIBC_2.2.5 (in /lib64/tls/libpthread-2.3.5.so)
==12739==    by 0x92DBED1: pthread_create@* hg_intercepts.c:213
==12739==    by 0x41622D: MyThread::Start(void*) thread_wrappers_pthread.h:242
==12739==    by 0x4081E0: test301::Run() racecheck_unittest.cc:4617
==12739==    by 0x4170BA: Test::Run() racecheck_unittest.cc:142
==12739==    by 0x40F099: main racecheck_unittest.cc:192
==12739==
==12739== T4: Possible data race during write of size 4 at 0x6277A4
==12739==    at 0x405BB6: test301::Worker2() racecheck_unittest.cc:4611
==12739==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==12739==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==12739==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==12739==   old state = {Mod; #LS=1; #SS=1; T3/S5}
==12739==   new state = {Mod; #LS=0; #SS=2; T3/S5, T4/S9}
==12739==   Last consistently used lock for 0x6277A4 was first observed
==12739==    at 0x92DBDBC: pthread_mutex_init hg_intercepts.c:346
==12739==    by 0x414E17: Mutex::Mutex() thread_wrappers_pthread.h:149
==12739==    by 0x404326: __static_initialization_and_destruction_0(int, int) racecheck_unittest.cc:4608
==12739==    by 0x4046D8: _GLOBAL__I_RunningOnValgrind racecheck_unittest.cc:5263
==12739==    by 0x417619: (within a.out)
==12739==    by 0x40180A: (within a.out)
==12739==    by 0x4175B9: __libc_csu_init (in a.out)
==12739==    by 0x417570: __libc_csu_init (in a.out)
==12739==   Address 0x6277A4 is 0 bytes inside data symbol "_ZN7test3014GLOBE"
==12739==
...
==12739== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 2 from 1)
}}}

==Understanding the report==
So, what information do we get from Helgrind? 

=== Threads ===
First of all, we get the information about the threads involved in the data race. 
For each thread a stack trace (context) of it's creation is printed. For each thread this is printed just once. 
{{{
==12739== Thread T3 was created
==12739==    at 0x9F4EE60: clone (in /lib64/tls/libc-2.3.5.so)
...
==12739==    by 0x4170BA: Test::Run() racecheck_unittest.cc:142
==12739==    by 0x40F099: main racecheck_unittest.cc:192
==12739==
==12739== Thread T4 was created
==12739==    at 0x9F4EE60: clone (in /lib64/tls/libc-2.3.5.so)
...
==12739==    by 0x4170BA: Test::Run() racecheck_unittest.cc:142
==12739==    by 0x40F099: main racecheck_unittest.cc:192

}}}

=== Data race == 
The report about the data race itself starts with 
{{{
==12739== T4: Possible data race during write of size 4 at 0x6277A4
}}}
Here we have the thread id `T4`, access type (`read` or `write`), size in bytes, and the address of the memory location. 

Next, the stack trace (context) of the access follows: 
{{{
==12739==    at 0x405BB6: test301::Worker2() racecheck_unittest.cc:4611
==12739==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==12739==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==12739==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
}}}

After the access context we get the old and the new states of this memory location (for details about states refer to [MSMProp1]).
{{{
==12739==   old state = {Mod; #LS=1; #SS=1; T3/S5}
==12739==   new state = {Mod; #LS=0; #SS=2; T3/S5, T4/S9}
}}}
In this particular report we see that before the last access the state contains only one thread segment and now it has two thread segments. 
Since the new state is `Write` and the lockset is empty, we have a race. 

Next line shows whether this memory location has been procted by any lock. 
In this case, this is the lock `MU1`, the report gives the context of `MU1` creation.
{{{
==12739==   Last consistently used lock for 0x6277A4 was first observed
==12739==    at 0x92DBDBC: pthread_mutex_init hg_intercepts.c:346
==12739==    by 0x414E17: Mutex::Mutex() thread_wrappers_pthread.h:149
==12739==    by 0x404326: __static_initialization_and_destruction_0(int, int) racecheck_unittest.cc:4608
...
}}}

The last line shows us the debug information for the memory location. If we are lucky to get a race on a global variable, we will see it's name. 
{{{
==12739==   Address 0x6277A4 is 0 bytes inside data symbol "_ZN7test3014GLOBE"
}}}

=Using traces (when the data race happens twice)=
So, in the the first example the data race is quite obvious from the report (isn't it?). 
We have a global variable which is not protected by any common lock. 

However, in most real cases the racey memory is hidden somewhere deep inside 
the data structures and accessed from many different places. 
Analyzing the data race by just looking at the race context is impossible. 
However, if the race on the given memory location happens at least twice, 
we can use *access traces* to understand the race. 

== Source code ==
In the following test we have many memory accesses and much less obvious race (one access is mistakenly protected by a wrong lock). 
{{{
4625 // In this test we have many different accesses to GLOB and only one access
4626 // is not synchronized properly.
4627 int     GLOB = 0;
4628
4629 Mutex MU1;
4630 Mutex MU2;
4631 void Worker() {
4632   for(int i = 0; i < 100; i++) {
4633     switch(i % 4) {
4634       case 0:
4635         // This read is protected correctly.
4636         MU1.Lock(); CHECK(GLOB >= 0); MU1.Unlock();
4637         break;
4638       case 1:
4639         // Here we used the wrong lock! The reason of the race is here.
4640         MU2.Lock(); CHECK(GLOB >= 0); MU2.Unlock();
4641         break;
4642       case 2:
4643         // This read is protected correctly.
4644         MU1.Lock(); CHECK(GLOB >= 0); MU1.Unlock();
4645         break;
4646       case 3:
4647         // This write is protected correctly.
4648         MU1.Lock(); GLOB++; MU1.Unlock();
4649         break;
4650     }
4651     // sleep a bit so that the threads interleave
4652     // and the race happens at least twice.
4653     usleep(100);
4654   }
4655 }
4656
4657 void Run() {
4658   printf("test302: Complex race that happens twice.\n");
4659   MyThread t1(Worker), t2(Worker);
4660   t1.Start();
4661   t2.Start();
4662   t1.Join();   t2.Join();
4663 }
}}}

== Running Helgrind == 
{{{
% valgrind --tool=helgrind ./a.out 302
==7969== Thread T3 was created
...
==7969== Thread T4 was created
...
==7969== T3: Possible data race during write of size 4 at 0x62787C
==7969==    at 0x405B7D: test302::Worker() racecheck_unittest.cc:4648
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==   old state = {RdO; #LS=0; #SS=2; T4/S13, T3/S15}
==7969==   new state = {Mod; #LS=0; #SS=2; T4/S13, T3/S15}
...
==7969==   Last consistently used lock for 0x62787C was first observed
==7969==    at 0x92DBDBC: pthread_mutex_init hg_intercepts.c:346
==7969==    by 0x414E17: Mutex::Mutex() thread_wrappers_pthread.h:149
==7969==    by 0x404399: __static_initialization_and_destruction_0(int, int) racecheck_unittest.cc:4630
...
==7969==   Address 0x62787C is 0 bytes inside data symbol "_ZN7test3024GLOBE"
==7969==
==7969== TRACE[1] {{{: Access{T3/S15 wr 0x62787C} -> new State{Mod; #LS=1; #SS=1; TR; T3/S15}
==7969==  Access stack trace:
==7969==    at 0x405B7D: test302::Worker() racecheck_unittest.cc:4648
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==  Locks held:
==7969==    Lock located at 0x6278B0 and first observed
==7969==    at 0x92D91A5: pthread_mutex_lock hg_intercepts.c:408
==7969==    by 0x415B18: Mutex::Lock() thread_wrappers_pthread.h:156
==7969==    by 0x405B73: test302::Worker() racecheck_unittest.cc:4648
...
==7969== }}}
==7969==
==7969== TRACE[2] {{{: Access{T3/S19 rd 0x62787C} -> new State{RdO; #LS=1; #SS=1; TR; T3/S19}
==7969==  Access stack trace:
==7969==    at 0x405AC6: test302::Worker() racecheck_unittest.cc:4636
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==  Locks held:
==7969==    Lock located at 0x6278B0 and first observed
==7969==    at 0x92D91A5: pthread_mutex_lock hg_intercepts.c:408
==7969==    by 0x415B18: Mutex::Lock() thread_wrappers_pthread.h:156
==7969==    by 0x405AC5: test302::Worker() racecheck_unittest.cc:4636
...
==7969== }}}
==7969==
==7969== TRACE[3] {{{: Access{T4/S17 rd 0x62787C} -> new State{RdO; #LS=1; #SS=2; TR; T4/S17, T3/S19}
==7969==  Access stack trace:
==7969==    at 0x405B3B: test302::Worker() racecheck_unittest.cc:4644
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==  Locks held:
==7969==    Lock located at 0x6278B0 and first observed
==7969==    at 0x92D91A5: pthread_mutex_lock hg_intercepts.c:408
==7969==    by 0x415B18: Mutex::Lock() thread_wrappers_pthread.h:156
==7969==    by 0x405B3A: test302::Worker() racecheck_unittest.cc:4644
...
==7969== }}}
==7969==
==7969== TRACE[4] {{{: Access{T3/S21 rd 0x62787C} -> new State{RdO; #LS=0; #SS=2; TR; T4/S17, T3/S21}
==7969==  Access stack trace:
==7969==    at 0x405B02: test302::Worker() racecheck_unittest.cc:4640
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==  Locks held:
==7969==    Lock located at 0x627910 and first observed
==7969==    at 0x92D91A5: pthread_mutex_lock hg_intercepts.c:408
==7969==    by 0x415B18: Mutex::Lock() thread_wrappers_pthread.h:156
==7969==    by 0x405B01: test302::Worker() racecheck_unittest.cc:4640
...
==7969== }}}
==7969==
==7969== TRACE[5] {{{: Access{T4/S23 rd 0x62787C} -> new State{RdO; #LS=0; #SS=2; TR; T3/S21, T4/S23}
==7969==  Access stack trace:
==7969==    at 0x405B74: test302::Worker() racecheck_unittest.cc:4648
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==  Locks held:
==7969==    Lock located at 0x6278B0 and first observed
==7969==    at 0x92D91A5: pthread_mutex_lock hg_intercepts.c:408
==7969==    by 0x415B18: Mutex::Lock() thread_wrappers_pthread.h:156
==7969==    by 0x405B73: test302::Worker() racecheck_unittest.cc:4648
...
==7969== }}}
==7969==
==7969== TRACE[6] {{{: Access{T4/S23 wr 0x62787C} -> new State{Mod; #LS=0; #SS=2; TR; T3/S21, T4/S23}
==7969==  Access stack trace:
==7969==    at 0x405B7D: test302::Worker() racecheck_unittest.cc:4648
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==  Locks held:
==7969==    Lock located at 0x6278B0 and first observed
==7969==    at 0x92D91A5: pthread_mutex_lock hg_intercepts.c:408
==7969==    by 0x415B18: Mutex::Lock() thread_wrappers_pthread.h:156
==7969==    by 0x405B73: test302::Worker() racecheck_unittest.cc:4648
...
==7969== }}}
==7969==
==7969== Race on 0x62787C is found again after 6 accesses. Last access:
==7969== TRACE[6] {{{: Access{T4/S23 wr 0x62787C} -> new State{Mod; #LS=0; #SS=2; TR; T3/S21, T4/S23}
==7969==  Access stack trace:
==7969==    at 0x405B7D: test302::Worker() racecheck_unittest.cc:4648
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==  Locks held:
==7969==    Lock located at 0x6278B0 and first observed
==7969==    at 0x92D91A5: pthread_mutex_lock hg_intercepts.c:408
==7969==    by 0x415B18: Mutex::Lock() thread_wrappers_pthread.h:156
==7969==    by 0x405B73: test302::Worker() racecheck_unittest.cc:4648
...
==7969== }}}
==7969==
==7969==
==7969== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 7 from 3)
}}}

== Understanding the traces ==
So, the race report itself gives us very little information: 
{{{
==7969== T3: Possible data race during write of size 4 at 0x62787C
==7969==    at 0x405B7D: test302::Worker() racecheck_unittest.cc:4648
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==   old state = {RdO; #LS=0; #SS=2; T4/S13, T3/S15}
==7969==   new state = {Mod; #LS=0; #SS=2; T4/S13, T3/S15}
...
==7969==   Last consistently used lock for 0x62787C was first observed
==7969==    at 0x92DBDBC: pthread_mutex_init hg_intercepts.c:346
==7969==    by 0x414E17: Mutex::Mutex() thread_wrappers_pthread.h:149
==7969==    by 0x404399: __static_initialization_and_destruction_0(int, int) racecheck_unittest.cc:4630
...
}}}

We see that the racey memory is `GLOB`, but we don't see why there is a race: this access to `GLOB` is protected by a lock. 
But once the race is reported, Helgrind enables *access tracing* for this particular memory location.
The state of this memory location is reset to `{Accesstype,currS, currLS}`, i.e. all the previous accesses are forgotten. 

Each trace contains:
  * Access information: `Access{T3/S15 wr 0x62787C}` (thread/segment, access type, address)
  * State of the memory location (right after the access): `State = State{Mod; #LS=1; #SS=1; TR; T3/S15}` (See MSMHelgrind340).
  * Access stack trace (context).
  * List of locks held during the access (if the access is `read`, RW- and R-locks are shown; for `write` only RW-locks are shown). For each lock the report shows the context of the lock acquisition.

In this test case we are lucky -- the race happened twice: 
{{{
==7969== Race on 0x62787C is found again after 6 accesses.
}}}
After the race has been found twice, the memory tracing is disabled forever for this memory location. 


Let's look at the lines starting with `TRACE`: 
{{{
1: TRACE[1] {{{: Access{T3/S15 wr 0x62787C} -> new State{Mod; #LS=1; #SS=1; TR; T3/S15}
2: TRACE[2] {{{: Access{T3/S19 rd 0x62787C} -> new State{RdO; #LS=1; #SS=1; TR; T3/S19}
3: TRACE[3] {{{: Access{T4/S17 rd 0x62787C} -> new State{RdO; #LS=1; #SS=2; TR; T4/S17, T3/S19}
4: TRACE[4] {{{: Access{T3/S21 rd 0x62787C} -> new State{RdO; #LS=0; #SS=2; TR; T4/S17, T3/S21}
5: TRACE[5] {{{: Access{T4/S23 rd 0x62787C} -> new State{RdO; #LS=0; #SS=2; TR; T3/S21, T4/S23}
6. TRACE[6] {{{: Access{T4/S23 wr 0x62787C} -> new State{Mod; #LS=0; #SS=2; TR; T3/S21, T4/S23}
}}}

We see that the Lockset became empty after the access 4 (access 3 has `#LS=1`, access 4 has `#LS=0`).
Let's look at the accesses 3 and 4: 
{{{
==7969== TRACE[3] {{{: Access{T4/S17 rd 0x62787C} -> new State{RdO; #LS=1; #SS=2; TR; T4/S17, T3/S19}
==7969==  Access stack trace:
==7969==    at 0x405B3B: test302::Worker() racecheck_unittest.cc:4644
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==  Locks held:
==7969==    Lock located at 0x6278B0 and first observed
==7969==    at 0x92D91A5: pthread_mutex_lock hg_intercepts.c:408
==7969==    by 0x415B18: Mutex::Lock() thread_wrappers_pthread.h:156
==7969==    by 0x405B3A: test302::Worker() racecheck_unittest.cc:4644
...
==7969== }}}
==7969==
==7969== TRACE[4] {{{: Access{T3/S21 rd 0x62787C} -> new State{RdO; #LS=0; #SS=2; TR; T4/S17, T3/S21}
==7969==  Access stack trace:
==7969==    at 0x405B02: test302::Worker() racecheck_unittest.cc:4640
==7969==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==7969==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==7969==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==7969==  Locks held:
==7969==    Lock located at 0x627910 and first observed
==7969==    at 0x92D91A5: pthread_mutex_lock hg_intercepts.c:408
==7969==    by 0x415B18: Mutex::Lock() thread_wrappers_pthread.h:156
==7969==    by 0x405B01: test302::Worker() racecheck_unittest.cc:4640
...
==7969== }}}
}}}

We see that during these two accesses we hold two different locks (`MU1` and `MU2`).
We are done! 


Note, that even though the lock was lost during the access 4, 
we can not report a race untill access 6 since reading shared data w/o a lock is not a crime. 


=Using annotations (when the data race happens once)=
Ok, now we know how to analyze races that happen twice (according to TODO 80% of races do happen twice).
But what shall we do if a race never repeats? 

In some cases, source code annotations and a second run of Helgrind helps! 

== Source code == 
Suppose we have a real large application and some memory 
location is accessed once w/o a proper synchronization 
(and many times with proper synchronization).
How to find this unsynchronized access?

{{{
4670 int     GLOB = 0;
4671
4672 Mutex MU;
4673 void Worker1() { CHECK(GLOB >= 0); }
4674 void Worker2() { MU.Lock(); GLOB=1;  MU.Unlock();}
4675
4676 void Run() {
4677   printf("test303: a race that needs annotations.\n");
4678   ANNOTATE_TRACE_MEMORY(&GLOB);
4679   MyThreadArray t(Worker1, Worker2);
4680   t.Start();
4681   t.Join();
4682 }
}}}


== Running Helgrind == 
{{{
% valgrind --tool=helgrind ./a.out 303
==9432== Thread T3 was created
...
==9432== Thread T4 was created
...
==9432== T4: Possible data race during write of size 4 at 0x62791C
==9432==    at 0x405A4A: test303::Worker2() racecheck_unittest.cc:4674
==9432==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==9432==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==9432==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==9432==   old state = {RdO; #LS=0; #SS=1; T3/S5}
==9432==   new state = {Mod; #LS=0; #SS=2; T3/S5, T4/S7}
==9432==   Location 0x62791C has never been protected by any lock
==9432==   Address 0x62791C is 0 bytes inside data symbol "_ZN7test3034GLOBE"
...
==9432== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 3 from 1)
}}}

==Annotating the source code and rerunning Helgrind== 
The report gave us the racey memory location 
(in this case it is a global variable, 
but it could be a field in a heap allocated structure or anything else). 
In many cases the simplets way to analyze the race is to annotate the racey address 
and rerun Helgrind with `--trace-level=2`.

Note that our example is already annotated: 
{{{
4678   ANNOTATE_TRACE_MEMORY(&GLOB);
}}}
The annotation is an executable code and should be inserted before the first use of this memory location (when possible).

So, let's rerun Helgrind:
{{{
% valgrind --tool=helgrind --trace-level=2 ./a.out 303
==9536== ENABLED TRACE {{{: 0x62793C; S2/T1
==9536==    at 0x92DC6E5: AnnotateTraceMemory hg_annotations.c:285
==9536==    by 0x408CF8: test303::Run() racecheck_unittest.cc:4678
==9536==    by 0x4170BA: Test::Run() racecheck_unittest.cc:142
==9536==    by 0x40F099: main racecheck_unittest.cc:192
==9536== }}}
...
==9536== TRACE[1] {{{: Access{T3/S5 rd 0x62793C} -> new State{RdO; #LS=0; #SS=1; T3/S5}
==9536==  Access stack trace:
==9536==    at 0x402100: test303::Worker1() racecheck_unittest.cc:4673
==9536==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==9536==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==9536==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==9536==  Locks held:
==9536==    None
==9536== }}}
...
==9536== TRACE[2] {{{: Access{T4/S7 wr 0x62793C} -> new State{Mod; #LS=0; #SS=2; T3/S5, T4/S7}
==9536==  Access stack trace:
==9536==    at 0x405A4A: test303::Worker2() racecheck_unittest.cc:4674
==9536==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==9536==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==9536==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==9536==  Locks held:
==9536==    Lock located at 0x627970 and first observed
==9536==    at 0x92D91A5: pthread_mutex_lock hg_intercepts.c:408
==9536==    by 0x415B18: Mutex::Lock() thread_wrappers_pthread.h:156
==9536==    by 0x405A49: test303::Worker2() racecheck_unittest.cc:4674
==9536==    by 0x92DBFEE: mythread_wrapper hg_intercepts.c:193
==9536==    by 0x94FCF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==9536==    by 0x9F4EEA1: clone (in /lib64/tls/libc-2.3.5.so)
==9536== }}}
...
==9536== T4: Possible data race during write of size 4 at 0x62793C
==9536==    at 0x405A4A: test303::Worker2() racecheck_unittest.cc:4674
...
}}}

So, we see the access traces on each access after 
the statement `ANNOTATE_TRACE_MEMORY(&GLOB);` has been executed. 

Looking at these two traces we see that the first access was made 
while not holding any lock. We are done! 


=Using segments (when the data race happens once and annotations do not help)=

Unfortunately, annotating the racey memory location is 
not always possible (e.g. if a memory location is some field 
inside an STL object). 


== Source code == 
{{{
3714 string *STR;
3715
3716 void Worker1() {
3717   sleep(0);
3718   MU.Lock(); CHECK(STR->length() >= 4); MU.Unlock();
3719 }
3720 void Worker2() {
3721   sleep(1);
3722   CHECK(STR->length() >= 4); // Unprotected!
3723 }
3724 void Worker3() {
3725   sleep(2);
3726   MU.Lock(); CHECK(STR->length() >= 4); MU.Unlock();
3727 }
3728 void Worker4() {
3729   sleep(3);
3730   MU.Lock(); *STR += " + a very very long string"; MU.Unlock();
3731 }
3732
3733 void Run() {
3734   STR = new string ("The String");
3735   printf("test304: a race where memory tracing does not work.\n");
3736   MyThreadArray t(Worker1, Worker2, Worker3, Worker4);
3737   t.Start();
3738   t.Join();
3739
3740   printf("%s\n", STR->c_str());
3741   delete STR;
3742 }
}}}

==Running Helgrind==
{{{
% valgrind --tool=helgrind  ./a.out 304
==17248== Thread #2 was created
==17248==    at 0x3D840B5E60: clone (in /lib64/tls/libc-2.3.5.so)
==17248==    by 0x4C2B0D3: do_clone (in /lib64/tls/libpthread-2.3.5.so)
==17248==    by 0x4C2BAE1: pthread_create@@GLIBC_2.2.5 (in /lib64/tls/libpthread-2.3.5.so)
==17248==    by 0x4A0A601: pthread_create@* hg_intercepts.c:213
==17248==    by 0x40DFB5: MyThread::Start(void*) thread_wrappers_pthread.h:242
==17248==    by 0x40E013: MyThreadArray::Start() racecheck_unittest.cc:179
==17248==    by 0x402CDF: test304::Run() racecheck_unittest.cc:3737
==17248==    by 0x40A4CE: main racecheck_unittest.cc:144
==17248==
==17248== Thread #4 was created
==17248==    at 0x3D840B5E60: clone (in /lib64/tls/libc-2.3.5.so)
==17248==    by 0x4C2B0D3: do_clone (in /lib64/tls/libpthread-2.3.5.so)
==17248==    by 0x4C2BAE1: pthread_create@@GLIBC_2.2.5 (in /lib64/tls/libpthread-2.3.5.so)
==17248==    by 0x4A0A601: pthread_create@* hg_intercepts.c:213
==17248==    by 0x40DFB5: MyThread::Start(void*) thread_wrappers_pthread.h:242
==17248==    by 0x40E013: MyThreadArray::Start() racecheck_unittest.cc:179
==17248==    by 0x402CDF: test304::Run() racecheck_unittest.cc:3737
==17248==    by 0x40A4CE: main racecheck_unittest.cc:144
==17248==
==17248== Thread #5 was created
==17248==    at 0x3D840B5E60: clone (in /lib64/tls/libc-2.3.5.so)
==17248==    by 0x4C2B0D3: do_clone (in /lib64/tls/libpthread-2.3.5.so)
==17248==    by 0x4C2BAE1: pthread_create@@GLIBC_2.2.5 (in /lib64/tls/libpthread-2.3.5.so)
==17248==    by 0x4A0A601: pthread_create@* hg_intercepts.c:213
==17248==    by 0x40DFB5: MyThread::Start(void*) thread_wrappers_pthread.h:242
==17248==    by 0x40E013: MyThreadArray::Start() racecheck_unittest.cc:179
==17248==    by 0x402CDF: test304::Run() racecheck_unittest.cc:3737
==17248==    by 0x40A4CE: main racecheck_unittest.cc:144
==17248==
==17248== T6: Possible data race during write of size 8 at 0x534D778
==17248==    at 0x4ED69D3: std::string::reserve(unsigned long) 
==17248==    by 0x4ED6BE5: std::string::append(char const*, unsigned long) 
==17248==    by 0x404562: test304::Worker4() racecheck_unittest.cc:3730
==17248==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==17248==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==17248==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==17248==   old state = {Read; #SS=4; #LS=0; S2/T2, S7/T4, S9/T5, S11/T6}
==17248==   new state = {Write; #SS=4; #LS=0; S2/T2, S7/T4, S9/T5, S11/T6}
==17248==   Last consistently used lock for 0x534D778 was first observed
==17248==    at 0x4A0A4EC: pthread_mutex_init hg_intercepts.c:346
==17248==    by 0x40D2F1: Mutex::Mutex() thread_wrappers_pthread.h:149
==17248==    by 0x401E44: __static_initialization_and_destruction_0(int, int) racecheck_unittest.cc:85
==17248==    by 0x402A76: _GLOBAL__I_MU racecheck_unittest.cc:3745
==17248==    by 0x40FC69: (within a.out)
==17248==    by 0x401552: (within a.out)
==17248==    by 0x40FBC7: (within a.out)
==17248==    by 0x40FC10: __libc_csu_init (in a.out)
==17248==    by 0x3D8401CA75: (below main) (in /lib64/tls/libc-2.3.5.so)
=17248==   Location 0x534D778 is 0 bytes inside a block of size 8 alloc'd
==17248==    at 0x4A06FF9: operator new(unsigned long) vg_replace_malloc.c:230
==17248==    by 0x402C73: test304::Run() racecheck_unittest.cc:3734
==17248==    by 0x40A4CE: main racecheck_unittest.cc:144
==17248==
==17248== TRACE ...
==17248==  Locks held:
==17248==    L:0x402114580/0x618950
==17248==    at 0x4A078D5: pthread_mutex_lock /home/kcc/valgrind/hgdev/helgrind/hg_intercepts.c:408
==17248==    by 0x40E110: Mutex::Lock() thread_wrappers_pthread.h:156
==17248==    by 0x404551: test304::Worker4() racecheck_unittest.cc:3730
==17248==    by 0x4A0A71E: mythread_wrapper /home/kcc/valgrind/hgdev/helgrind/hg_intercepts.c:193
==17248==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==17248==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==17248== }}}

}}}

The key information in the report is contained in these two lines: 
{{{
==17248==   old state = {Read; #SS=4; #LS=0; S2/T2, S7/T4, S9/T5, S11/T6}
==17248==   new state = {Write; #SS=4; #LS=0; S2/T2, S7/T4, S9/T5, S11/T6}
}}}
These lines should be read like this: 
  * The memory location has been accessed from four different threads (T2, T4, T5, T6).
  * All but the last accesses were reads.
  * The accesses do not share any common lock. 

Does this information help us? Not really. 
We know more or less everything about the last access (the context and the locks held)
but the last access seems to be correctly synchronized.
There were three other accesses in threads T2, T4, T5, and one of them is buggy. 
If we knew the context of these 3 accesses, it would be possible to analyze the source code manually and check the synchronization. 
But the object might be used in hundreds of different places throghout the program! 


==Rerunning with more segments==
*Right now we have a very subtle machinery that helps (sometimes) find the accesses involved in the data race. 
We either need to improve this technique significantly, or create something else. *

Suppose we can find all accesses to the racey object. 
Then we chould annotate each such access with `ANNOTATE_CONDVAR_SIGNAL(some_constant)`. Like this: 
{{{
3716 void Worker1() {
3717   sleep(0);
3718   ANNOTATE_CONDVAR_SIGNAL((void*)0xDEADBEAF);
3719   MU.Lock(); CHECK(STR->length() >= 4); MU.Unlock();
3720 }
3721 void Worker2() {
3722   sleep(1);
3723   ANNOTATE_CONDVAR_SIGNAL((void*)0xDEADBEAF);
3724   CHECK(STR->length() >= 4); // Unprotected!
3725 }
3726 void Worker3() {
3727   sleep(2);
3728   ANNOTATE_CONDVAR_SIGNAL((void*)0xDEADBEAF);
3729   MU.Lock(); CHECK(STR->length() >= 4); MU.Unlock();
3730 }
3731 void Worker4() {
3732   sleep(3);
3733   ANNOTATE_CONDVAR_SIGNAL((void*)0xDEADBEAF);
3734   MU.Lock(); *STR += " + a very very long string"; MU.Unlock();
3735 }
}}}

And rerun Helgrind: 
{{{
% valgrind --tool=helgrind  ./a.out 304
==21495== T6: Possible data race during write of size 8 at 0x534D778
==21495==    at 0x4ED69D3: std::string::reserve(unsigned long) 
==21495==    by 0x4ED6BE5: std::string::append(char const*, unsigned long) 
==21495==    by 0x4045E4: test304::Worker4() racecheck_unittest.cc:3734
==21495==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==21495==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==21495==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==21495==   old state = {Read; #SS=4; #LS=0; S4/T2, S16/T4, S19/T5, S24/T6}
==21495==   new state = {Write; #SS=4; #LS=0; S4/T2, S16/T4, S19/T5, S24/T6}
==21495==  SS4/T2:
==21495==    at 0x4A0A924: AnnotateCondVarSignal hg_annotations.c:174
==21495==    by 0x404677: test304::Worker1() racecheck_unittest.cc:3718
==21495==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==21495==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==21495==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==21495==  SS16/T4:
==21495==    at 0x4A0A924: AnnotateCondVarSignal hg_annotations.c:174
==21495==    by 0x404505: test304::Worker2() racecheck_unittest.cc:3723
==21495==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==21495==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==21495==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==21495==  SS19/T5:
==21495==    at 0x4A0A924: AnnotateCondVarSignal hg_annotations.c:174
==21495==    by 0x404613: test304::Worker3() racecheck_unittest.cc:3728
==21495==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==21495==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==21495==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==21495==  SS24/T6:
==21495==    at 0x4A0A924: AnnotateCondVarSignal hg_annotations.c:174
==21495==    by 0x4045C9: test304::Worker4() racecheck_unittest.cc:3733
==21495==    by 0x4A0A71E: mythread_wrapper hg_intercepts.c:193
==21495==    by 0x4C2AF9E: start_thread (in /lib64/tls/libpthread-2.3.5.so)
==21495==    by 0x3D840B5EA1: clone (in /lib64/tls/libc-2.3.5.so)
==21495==   Last consistently used lock for 0x534D778 was first observed
==21495==    at 0x4A0A4EC: pthread_mutex_init hg_intercepts.c:346
==21495==    by 0x40D341: Mutex::Mutex() thread_wrappers_pthread.h:149
==21495==    by 0x401E44: __static_initialization_and_destruction_0(int, int) racecheck_unittest.cc:85
==21495==    by 0x402A76: _GLOBAL__I_MU racecheck_unittest.cc:3749
==21495==    by 0x40FCB9: (within a.out)
==21495==    by 0x401552: (within a.out)
==21495==    by 0x40FC17: (within a.out)
==21495==    by 0x40FC60: __libc_csu_init (in a.out)
==21495==    by 0x3D8401CA75: (below main) (in /lib64/tls/libc-2.3.5.so)
==21495==   Location 0x534D778 is 0 bytes inside a block of size 8 alloc'd
==21495==    at 0x4A06FF9: operator new(unsigned long) 
==21495==    by 0x402C73: test304::Run() racecheck_unittest.cc:3738
==21495==    by 0x40A51E: main racecheck_unittest.cc:144
==21495==
}}}

This report shows the contexts of creation for each segment involved in the data race. 

*TODO*: we may need to remember the locksets during segment creation.


*TODO*: What if we can not find all the accesses to the object and 
insert an annotations before each access? 
Just insert annotations everywhere! 
Automatically? Using some kind of binary search? 


TODO


=Possible improvements=
  * Implement Helgrind client requests to attach symbolic names to threads and locks. 
  * TODO